Decision Trees: allow you to ask multiple linear questions one after the other

min_samples_split: The minimum number of samples required to split an internal node
			* If is small we can over fit our data, increasing this value lets us generalize the data.
					  
Def Entropy: Measures the IMPURITY in a bunch of examples 
			* Controls how a decision tree (DT) decides where to split the data.
	 
	eg: All examples are SAME CLASS, then entropy = 0 
	eg: All examples are EVENLY SPLIT between classes, then entropy = 1
	
Entropy = - âˆ‘ p_i log_2 (p_i)

Information Gain = entropy(parent) - [weighted average]entropy(children)
		                              ^^^^^^^^^^^^ this means the weighted average of the entropy of the children

The decision tree algorithm will maximize information gain.

Pros: Build bigger classifier out of DT
Cons: Prone to over fitting 
